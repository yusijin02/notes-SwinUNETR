# [è®ºæ–‡ç¬”è®°] Swin UNETR è®ºæ–‡ç¬”è®°: MRI å›¾åƒè„‘è‚¿ç˜¤è¯­ä¹‰åˆ†å‰²

Author: <a href="https://yusijin02.github.io/">Sijin Yu</a>

[1] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R. Roth, and Daguang Xu. *Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images*. MICCAI, 2022.

<a href="https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR">ğŸ“å¼€æºä»£ç é“¾æ¥</a>

[TOC]

## 1. Abstract

- è„‘è‚¿ç˜¤çš„è¯­ä¹‰åˆ†å‰²æ˜¯ä¸€é¡¹åŸºæœ¬çš„åŒ»å­¦å½±åƒåˆ†æä»»åŠ¡, æ¶‰åŠå¤šç§ MRI æˆåƒæ¨¡æ€, å¯ååŠ©ä¸´åºŠåŒ»ç”Ÿè¯Šæ–­ç—…äººå¹¶éšåç ”ç©¶æ¶æ€§å®ä½“çš„è¿›å±•.
- è¿‘å¹´æ¥, å®Œå…¨**å·ç§¯ç¥ç»ç½‘ç»œ (Fully Convolutional Neural Networks, FCNNs)** æ–¹æ³•å·²æˆä¸º 3D åŒ»å­¦å½±åƒåˆ†å‰²çš„äº‹å®æ ‡å‡†.
- æµè¡Œçš„ â€œUå½¢â€ ç½‘ç»œæ¶æ„åœ¨ä¸åŒçš„ 2D å’Œ 3D è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä»¥åŠå„ç§æˆåƒæ¨¡å¼ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½åŸºå‡†.
- ç„¶è€Œ, ç”±äº FCNNs ä¸­å·ç§¯å±‚çš„æ ¸å¤§å°æœ‰é™, å®ƒä»¬åœ¨å»ºæ¨¡é•¿è·ç¦»ä¿¡æ¯æ–¹é¢çš„æ€§èƒ½æ˜¯æ¬¡ä¼˜çš„, è¿™å¯èƒ½å¯¼è‡´åœ¨åˆ†å‰²å¤§å°ä¸ä¸€çš„è‚¿ç˜¤æ—¶å‡ºç°ç¼ºé™·.
- å¦ä¸€æ–¹é¢, Transformer æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå±•ç¤ºäº†æ•è·é•¿è·ç¦»ä¿¡æ¯çš„å“è¶Šèƒ½åŠ›, åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰.
- å— ViT åŠå…¶å˜ä½“æˆåŠŸçš„å¯å‘, æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º **Swin UNEt TRansformers (Swin UNETR)** çš„æ–°å‹åˆ†å‰²æ¨¡å‹.
- å…·ä½“æ¥è¯´, 3D è„‘è‚¿ç˜¤è¯­ä¹‰åˆ†å‰²ä»»åŠ¡è¢«é‡æ–°å®šä¹‰ä¸º**åºåˆ—åˆ°åºåˆ—é¢„æµ‹é—®é¢˜**, å…¶ä¸­å¤šæ¨¡æ€è¾“å…¥æ•°æ®è¢«æŠ•å½±æˆä¸€ç»´åµŒå…¥åºåˆ—, å¹¶ç”¨ä½œå±‚çº§ Swin å˜æ¢å™¨ç¼–ç å™¨çš„è¾“å…¥.
- Swin Transformer ç¼–ç å™¨**ä½¿ç”¨ç§»ä½çª—å£è®¡ç®—è‡ªæ³¨æ„åŠ›**, åœ¨äº”ä¸ªä¸åŒçš„åˆ†è¾¨ç‡ä¸Šæå–ç‰¹å¾, å¹¶é€šè¿‡è·³è·ƒè¿æ¥åœ¨æ¯ä¸ªåˆ†è¾¨ç‡ä¸Šè¿æ¥åˆ°åŸºäºFCNN çš„è§£ç å™¨.
- æˆ‘ä»¬å‚åŠ äº† 2021 å¹´ BraTS åˆ†å‰²æŒ‘æˆ˜èµ›, æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨éªŒè¯é˜¶æ®µä½åˆ—è¡¨ç°æœ€ä½³çš„æ–¹æ³•ä¹‹ä¸€.

## 2. Motivation & Contribution

### 2.1 Motivation

- åœ¨åŒ»ç–—ä¿å¥çš„äººå·¥æ™ºèƒ½é¢†åŸŸ, ç‰¹åˆ«æ˜¯è„‘è‚¿ç˜¤åˆ†æä¸­, éœ€è¦æ›´å…ˆè¿›çš„åˆ†å‰²æŠ€æœ¯æ¥å‡†ç¡®åˆ’å®šè‚¿ç˜¤, ä»¥ä¾¿è¯Šæ–­å’Œæœ¯å‰è§„åˆ’.
- å½“å‰åŸºäº CNN çš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ç”±äºå…¶å°æ„Ÿå—é‡, éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³».
- ViTs åœ¨æ•æ‰å„ç§é¢†åŸŸçš„é•¿è·ç¦»ä¿¡æ¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›, æš—ç¤ºå…¶åœ¨æ”¹å–„åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é€‚ç”¨æ€§.

### 2.2 Contribution

- æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„, Swin UNEt TRansformers (Swin UNETR), ç»“åˆäº† Swin Transformer ç¼–ç å™¨ä¸ U å½¢ CNN è§£ç å™¨, ç”¨äºå¤šæ¨¡æ€ä¸‰ç»´è„‘è‚¿ç˜¤åˆ†å‰².
- åœ¨ 2021 å¹´å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜ (BraTS) ä¸­å±•ç¤ºäº† Swin UNETR æ¨¡å‹çš„æœ‰æ•ˆæ€§, éªŒè¯é˜¶æ®µå–å¾—äº†æ’åé å‰çš„æˆç»©, å¹¶åœ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›.

## 3. Model

![1](./img/1.png)

1. **å°†è¾“å…¥çš„å›¾åƒæ‰“æˆ Patch**.

   è¾“å…¥çš„å›¾åƒä¸º $X\in\mathbb R^{H\times W\times D\times S}$. ä¸€ä¸ª Patch çš„åˆ†è¾¨ç‡ä¸º $(H',W',D')$, ä¸€ä¸ª Patch çš„å½¢çŠ¶ä¸º $\mathbb R^{H'\times W'\times D'\times S}$.

   åˆ™å›¾åƒå˜ä¸ºä¸€ä¸ª Patch çš„åºåˆ—, åºåˆ—é•¿åº¦ä¸º $\lceil\frac{H}{H'}\rceil\times\lceil\frac{W}{W'}\rceil\times\lceil\frac{D}{D'}\rceil$.

   åœ¨æœ¬æ–‡ä¸­, Patch size ä¸º $(H',W',D')=(2, 2, 2)$.

   å¯¹äºæ¯ä¸ª patch, å°†å…¶æ˜ å°„ä¸ºä¸€ä¸ªåµŒå…¥ç»´åº¦ä¸º $C$  çš„ token. å› æ­¤, æœ€ç»ˆå¾—åˆ°åˆ†è¾¨ç‡ä¸º $(\lceil\frac{H}{H'}\rceil,\lceil\frac{W}{W'}\rceil,\lceil\frac{D}{D'}\rceil)$ çš„ 3D tokens. 

2. **å¯¹ 3D tokens åº”ç”¨ Swin Transformer**.

   ä¸€å±‚ Swin Transformer Block ç”±ä¸¤ä¸ªå­å±‚ç»„æˆ: W-MSA, SW-MSA.

   ç»è¿‡ä¸€å±‚ Swin Transformer Block, ä¸€ä¸ª 3D tokens æ¯ä¸ªæ–¹å‘ä¸Šçš„åˆ†è¾¨ç‡å˜ä¸ºåŸæ¥çš„ $\frac12$, é€šé“æ•°å˜ä¸ºåŸæ¥çš„ $2$ å€. è§ Fig.1 çš„å·¦ä¸‹è§’.

   W-MSA å’Œ SW-MSA åˆ†åˆ«æ˜¯è§„åˆ™çš„ã€å¾ªç¯ç§»åŠ¨çš„ partitioning multi-head self-attention, å¦‚ä¸‹å›¾æ‰€ç¤º.

   ![2](./img/2.png)

## 4. Experiment

### 4.1 Dataset

- BraTS 2021

### 4.2 å¯¹æ¯”å®éªŒ

![3](./img/3.png)

## 5. Code

ä»¥ä¸‹é“¾æ¥æä¾›äº†ä½¿ç”¨Swin UNETRæ¨¡å‹è¿›è¡ŒBraTS21è„‘è‚¿ç˜¤åˆ†å‰²çš„æ•™ç¨‹:[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb)

ä¸‹é¢æ˜¯éƒ¨åˆ†æ ¸å¿ƒä»£ç æ³¨é‡Š:

### 5.1 æ•°æ®é¢„å¤„ç†å’Œå¢å¼º

```Python
from monai import transforms

train_transform = transforms.Compose(
  [	
  	# è¯»å…¥å›¾åƒ
    transforms.LoadImaged(keys=["image", "label"]),
    
		# å°†å•é€šé“çš„æ ‡ç­¾å›¾åƒè½¬æ¢æˆå¤šé€šé“æ ¼å¼, æ¯ä¸ªé€šé“è¡¨ç¤ºä¸åŒçš„è‚¿ç˜¤ç±»åˆ«. (è½¬æ¢å‰æ˜¯æ‰€æœ‰ç±»åˆ«æ ‡ç­¾å›¾å…±ç”¨ä¸€ä¸ªå•é€šé“å›¾åƒ)    transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
		# è£å‰ªæ‰å›¾åƒå‘¨å›´çš„èƒŒæ™¯åŒºåŸŸ
    transforms.CropForegroundd(
        keys=["image", "label"],
        source_key="image",
        k_divisible=[roi[0], roi[1], roi[2]],
    ),
    # å°†å›¾åƒéšæœºè£å‰ªä¸ºæŒ‡å®šå¤§å°
    transforms.RandSpatialCropd(
        keys=["image", "label"],
        roi_size=[roi[0], roi[1], roi[2]],
        random_size=False,
    ),
    # åœ¨0è½´æ–¹å‘ä¸Šéšæœºç¿»è½¬
    transforms.RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=0),
    # åœ¨1è½´æ–¹å‘ä¸Šéšæœºç¿»è½¬
    transforms.RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=1),
    # åœ¨2è½´æ–¹å‘ä¸Šéšæœºç¿»è½¬
    transforms.RandFlipd(keys=["image", "label"], prob=0.5, spatial_axis=2),
    # å¯¹æ¯ä¸ªå•ç‹¬é€šé“, è¿›è¡Œå¼ºåº¦å½’ä¸€åŒ–, ä¸”å¿½ç•¥0å€¼
    transforms.NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
    # éšæœºè°ƒæ•´å›¾åƒçš„å¼ºåº¦, img = img * (1 + eps)
    transforms.RandScaleIntensityd(keys="image", factors=0.1, prob=1.0),
    # éšæœºè°ƒæ•´å›¾åƒçš„å¼ºåº¦, img = img + eps
    transforms.RandShiftIntensityd(keys="image", offsets=0.1, prob=1.0),
	]
)

val_transform = transforms.Compose(
	[
    transforms.LoadImaged(keys=["image", "label"]),
    transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys="label"),
    transforms.NormalizeIntensityd(keys="image", nonzero=True, channel_wise=True),
  ]
)
```

### 5.2 Swin UNETR æ¨¡å‹æ¶æ„

```Python
def forward(self, x_in):
  if not torch.jit.is_scripting():
    self._check_input_size(x_in.shape[2:])
  hidden_states_out = self.swinViT(x_in, self.normalize)
  enc0 = self.encoder1(x_in)
  enc1 = self.encoder2(hidden_states_out[0])
  enc2 = self.encoder3(hidden_states_out[1])
  enc3 = self.encoder4(hidden_states_out[2])
  dec4 = self.encoder10(hidden_states_out[4])
  dec3 = self.decoder5(dec4, hidden_states_out[3])
  dec2 = self.decoder4(dec3, enc3)
  dec1 = self.decoder3(dec2, enc2)
  dec0 = self.decoder2(dec1, enc1)
  out = self.decoder1(dec0, enc0)
  logits = self.out(out)
  return logits
```

ç»„ä»¶çš„å®šä¹‰å¦‚ä¸‹:

```Python
self.normalize = normalize

self.swinViT = SwinTransformer(
  in_chans=in_channels,
  embed_dim=feature_size,
  window_size=window_size,
  patch_size=patch_sizes,
  depths=depths,
  num_heads=num_heads,
  mlp_ratio=4.0,
  qkv_bias=True,
  drop_rate=drop_rate,
  attn_drop_rate=attn_drop_rate,
  drop_path_rate=dropout_path_rate,
  norm_layer=nn.LayerNorm,
  use_checkpoint=use_checkpoint,
  spatial_dims=spatial_dims,
  downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,
  use_v2=use_v2,
)

self.encoder1 = UnetrBasicBlock(
  spatial_dims=spatial_dims,
  in_channels=in_channels,
  out_channels=feature_size,
  kernel_size=3,
  stride=1,
  norm_name=norm_name,
  res_block=True,
)

self.encoder2 = UnetrBasicBlock(
  spatial_dims=spatial_dims,
  in_channels=feature_size,
  out_channels=feature_size,
  kernel_size=3,
  stride=1,
  norm_name=norm_name,
  res_block=True,
)

self.encoder3 = UnetrBasicBlock(
  spatial_dims=spatial_dims,
  in_channels=2 * feature_size,
  out_channels=2 * feature_size,
  kernel_size=3,
  stride=1,
  norm_name=norm_name,
  res_block=True,
)

self.encoder4 = UnetrBasicBlock(
  spatial_dims=spatial_dims,
  in_channels=4 * feature_size,
  out_channels=4 * feature_size,
  kernel_size=3,
  stride=1,
  norm_name=norm_name,
  res_block=True,
)

self.encoder10 = UnetrBasicBlock(
  spatial_dims=spatial_dims,
  in_channels=16 * feature_size,
  out_channels=16 * feature_size,
  kernel_size=3,
  stride=1,
  norm_name=norm_name,
  res_block=True,
)

self.decoder5 = UnetrUpBlock(
  spatial_dims=spatial_dims,
  in_channels=16 * feature_size,
  out_channels=8 * feature_size,
  kernel_size=3,
  upsample_kernel_size=2,
  norm_name=norm_name,
  res_block=True,
)

self.decoder4 = UnetrUpBlock(
  spatial_dims=spatial_dims,
  in_channels=feature_size * 8,
  out_channels=feature_size * 4,
  kernel_size=3,
  upsample_kernel_size=2,
  norm_name=norm_name,
  res_block=True,
)

self.decoder3 = UnetrUpBlock(
  spatial_dims=spatial_dims,
  in_channels=feature_size * 4,
  out_channels=feature_size * 2,
  kernel_size=3,
  upsample_kernel_size=2,
  norm_name=norm_name,
  res_block=True,
)
self.decoder2 = UnetrUpBlock(
  spatial_dims=spatial_dims,
  in_channels=feature_size * 2,
  out_channels=feature_size,
  kernel_size=3,
  upsample_kernel_size=2,
  norm_name=norm_name,
  res_block=True,
)

self.decoder1 = UnetrUpBlock(
  spatial_dims=spatial_dims,
  in_channels=feature_size,
  out_channels=feature_size,
  kernel_size=3,
  upsample_kernel_size=2,
  norm_name=norm_name,
  res_block=True,
)

self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)
```

#### 5.2.1 SwinTransformer

```python
class SwinTransformer(nn.Module):
  """
  Swin Transformer based on: "Liu et al.,
  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
  <https://arxiv.org/abs/2103.14030>"
  https://github.com/microsoft/Swin-Transformer
  """

  def __init__(
    self,
    in_chans: int,
    embed_dim: int,
    window_size: Sequence[int],
    patch_size: Sequence[int],
    depths: Sequence[int],
    num_heads: Sequence[int],
    mlp_ratio: float = 4.0,
    qkv_bias: bool = True,
    drop_rate: float = 0.0,
    attn_drop_rate: float = 0.0,
    drop_path_rate: float = 0.0,
    norm_layer: type[LayerNorm] = nn.LayerNorm,
    patch_norm: bool = False,
    use_checkpoint: bool = False,
    spatial_dims: int = 3,
    downsample="merging",
    use_v2=False,
  ) -> None:
  """
  Args:
    in_chans: dimension of input channels.
    embed_dim: number of linear projection output channels.
    window_size: local window size.
    patch_size: patch size.
    depths: number of layers in each stage.
    num_heads: number of attention heads.
    mlp_ratio: ratio of mlp hidden dim to embedding dim.
    qkv_bias: add a learnable bias to query, key, value.
    drop_rate: dropout rate.
    attn_drop_rate: attention dropout rate.
    drop_path_rate: stochastic depth rate.
    norm_layer: normalization layer.
    patch_norm: add normalization after patch embedding.
    use_checkpoint: use gradient checkpointing for reduced memory usage.
    spatial_dims: spatial dimension.
    downsample: module used for downsampling, available options are `"mergingv2"`, `"merging"` and a
        user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.
        The default is currently `"merging"` (the original version defined in v0.9.0).
    use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.
  """
    super().__init__()
    self.num_layers = len(depths)
    self.embed_dim = embed_dim
    self.patch_norm = patch_norm
    self.window_size = window_size
    self.patch_size = patch_size
    self.patch_embed = PatchEmbed(
        patch_size=self.patch_size,
        in_chans=in_chans,
        embed_dim=embed_dim,
        norm_layer=norm_layer if self.patch_norm else None,  # type: ignore
        spatial_dims=spatial_dims,
    )
    self.pos_drop = nn.Dropout(p=drop_rate)
    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
    self.use_v2 = use_v2
    self.layers1 = nn.ModuleList()
    self.layers2 = nn.ModuleList()
    self.layers3 = nn.ModuleList()
    self.layers4 = nn.ModuleList()
    if self.use_v2:
      self.layers1c = nn.ModuleList()
      self.layers2c = nn.ModuleList()
      self.layers3c = nn.ModuleList()
      self.layers4c = nn.ModuleList()
    down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample
    for i_layer in range(self.num_layers):
      layer = BasicLayer(
        dim=int(embed_dim * 2**i_layer),
        depth=depths[i_layer],
        num_heads=num_heads[i_layer],
        window_size=self.window_size,
        drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],
        mlp_ratio=mlp_ratio,
        qkv_bias=qkv_bias,
        drop=drop_rate,
        attn_drop=attn_drop_rate,
        norm_layer=norm_layer,
        downsample=down_sample_mod,
        use_checkpoint=use_checkpoint,
        )
      if i_layer == 0:
        self.layers1.append(layer)
      elif i_layer == 1:
        self.layers2.append(layer)
      elif i_layer == 2:
        self.layers3.append(layer)
      elif i_layer == 3:
        self.layers4.append(layer)
      if self.use_v2:
        layerc = UnetrBasicBlock(
          spatial_dims=3,
          in_channels=embed_dim * 2**i_layer,
          out_channels=embed_dim * 2**i_layer,
          kernel_size=3,
          stride=1,
          norm_name="instance",
          res_block=True,
        )
      if i_layer == 0:
        self.layers1c.append(layerc)
      elif i_layer == 1:
        self.layers2c.append(layerc)
      elif i_layer == 2:
        self.layers3c.append(layerc)
      elif i_layer == 3:
        self.layers4c.append(layerc)
    self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))

  def proj_out(self, x, normalize=False):
    if normalize:
      x_shape = x.size()
      if len(x_shape) == 5:
        n, ch, d, h, w = x_shape
        x = rearrange(x, "n c d h w -> n d h w c")
        x = F.layer_norm(x, [ch])
        x = rearrange(x, "n d h w c -> n c d h w")
      elif len(x_shape) == 4:
        n, ch, h, w = x_shape
        x = rearrange(x, "n c h w -> n h w c")
        x = F.layer_norm(x, [ch])
        x = rearrange(x, "n h w c -> n c h w")
    return x

  def forward(self, x, normalize=True):
    x0 = self.patch_embed(x)
    x0 = self.pos_drop(x0)
    x0_out = self.proj_out(x0, normalize)
    if self.use_v2:
      x0 = self.layers1c[0](x0.contiguous())
    x1 = self.layers1[0](x0.contiguous())
    x1_out = self.proj_out(x1, normalize)
    if self.use_v2:
      x1 = self.layers2c[0](x1.contiguous())
    x2 = self.layers2[0](x1.contiguous())
    x2_out = self.proj_out(x2, normalize)
    if self.use_v2:
      x2 = self.layers3c[0](x2.contiguous())
    x3 = self.layers3[0](x2.contiguous())
    x3_out = self.proj_out(x3, normalize)
    if self.use_v2:
      x3 = self.layers4c[0](x3.contiguous())
    x4 = self.layers4[0](x3.contiguous())
    x4_out = self.proj_out(x4, normalize)
    return [x0_out, x1_out, x2_out, x3_out, x4_out]
```

#### 5.2.2 UnetrBasicBlock

```Python
class UnetrBasicBlock(nn.Module):
  """
  A CNN module that can be used for UNETR, based on: "Hatamizadeh et al.,
  UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
  """

  def __init__(
    self,
    spatial_dims: int,
    in_channels: int,
    out_channels: int,
    kernel_size: Sequence[int] | int,
    stride: Sequence[int] | int,
    norm_name: tuple | str,
    res_block: bool = False,
  ) -> None:
    """
    Args:
      spatial_dims: number of spatial dimensions.
      in_channels: number of input channels.
      out_channels: number of output channels.
      kernel_size: convolution kernel size.
      stride: convolution stride.
      norm_name: feature normalization type and arguments.
      res_block: bool argument to determine if residual block is used.
    """

    super().__init__()

    if res_block:
      self.layer = UnetResBlock(
        spatial_dims=spatial_dims,
        in_channels=in_channels,
        out_channels=out_channels,
        kernel_size=kernel_size,
        stride=stride,
        norm_name=norm_name,
      )
    else:
      self.layer = UnetBasicBlock(  # type: ignore
        spatial_dims=spatial_dims,
        in_channels=in_channels,
        out_channels=out_channels,
        kernel_size=kernel_size,
        stride=stride,
        norm_name=norm_name,
      )

  def forward(self, inp):
    return self.layer(inp)
```

#### 5.2.3 UnetrUpBlock

```Python
class UnetrUpBlock(nn.Module):
  """
  An upsampling module that can be used for UNETR: "Hatamizadeh et al.,
  UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
  """

  def __init__(
    self,
    spatial_dims: int,
    in_channels: int,
    out_channels: int,
    kernel_size: Sequence[int] | int,
    upsample_kernel_size: Sequence[int] | int,
    norm_name: tuple | str,
    res_block: bool = False,
  ) -> None:
    """
    Args:
      spatial_dims: number of spatial dimensions.
      in_channels: number of input channels.
      out_channels: number of output channels.
      kernel_size: convolution kernel size.
      upsample_kernel_size: convolution kernel size for transposed convolution layers.
      norm_name: feature normalization type and arguments.
      res_block: bool argument to determine if residual block is used.
    """
    super().__init__()
    upsample_stride = upsample_kernel_size
    self.transp_conv = get_conv_layer(
      spatial_dims,
      in_channels,
      out_channels,
      kernel_size=upsample_kernel_size,
      stride=upsample_stride,
      conv_only=True,
      is_transposed=True,
    )

    if res_block:
      self.conv_block = UnetResBlock(
        spatial_dims,
        out_channels + out_channels,
        out_channels,
        kernel_size=kernel_size,
        stride=1,
        norm_name=norm_name,
      )
    else:
      self.conv_block = UnetBasicBlock(  # type: ignore
        spatial_dims,
        out_channels + out_channels,
        out_channels,
        kernel_size=kernel_size,
        stride=1,
        norm_name=norm_name,
      )

  def forward(self, inp, skip):
    # number of channels for skip should equals to out_channels
    out = self.transp_conv(inp)
    out = torch.cat((out, skip), dim=1)
    out = self.conv_block(out)
    return out
```

#### 5.2.4 UnetOutBlock

```Python
class UnetOutBlock(nn.Module):
  def __init__(
    self, spatial_dims: int, in_channels: int, out_channels: int, dropout: tuple | str | float | None = None
  ):
    super().__init__()
    self.conv = get_conv_layer(
      spatial_dims,
      in_channels,
      out_channels,
      kernel_size=1,
      stride=1,
      dropout=dropout,
      bias=True,
      act=None,
      norm=None,
      conv_only=False,
    )

  def forward(self, inp):
    return self.conv(inp)
```



















































